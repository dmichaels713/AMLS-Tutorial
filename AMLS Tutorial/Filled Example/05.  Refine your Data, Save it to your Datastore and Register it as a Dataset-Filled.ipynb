{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refine your Data, save it to your Datastore, and register it as a Dataset\n",
    "The point of refining your data is to create a version where everyone can easily use it.  First, load in your Workspace, Datastore and Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Azure Libaries\n",
    "from azureml.core import Datastore\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "from azure.storage.blob import BlockBlobService\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve your workspace by name by filling in the lower case values between double quotes\n",
    "ws = Workspace.get(name=\"ancient-rivers-ml-workspace\",\n",
    "        subscription_id=\"47a7ec0c-37ad-428b-9114-b87ea1057632\",\n",
    "        resource_group=\"xeek-ancient-rivers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve your Datastore by name by filling in the lower case values between double quotes\n",
    "datastore_name = \"ancientrivers\"\n",
    "datastore = Datastore.get(ws, datastore_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve your Datasets by name by filling in the lower case values between double quotes\n",
    "dataset_name_train = \"ancient_rivers_train_raw\"\n",
    "dataset_name_test  = \"ancient_rivers_test_raw\"\n",
    "\n",
    "# Load Data in as Tabular Datasets\n",
    "training_data = Dataset.get_by_name(ws, dataset_name_train, version='latest')\n",
    "testing_data  = Dataset.get_by_name(ws, dataset_name_test, version='latest')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert your tabular dataset to pandas data frames\n",
    "testDF = testing_data.to_pandas_dataframe()\n",
    "trainDF = training_data.to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# It's time to start working with your data!\n",
    "This next step should be customized to fit each project following the eight steps listed below.<br>\n",
    "You should understand your data, refine your data, and save it as a Dataset so everyone on your workspace can access it.<br>\n",
    "\n",
    "0.  Select columns that you will use in your model and order them correctly<br>\n",
    "1.  Rename Columns to be Comprehensible without Documentation<br>\n",
    "2.  Assign Correct Data Types to Each Column<br>\n",
    "3.  Fix Errors in the Data or Replace Errors with Nulls<br>\n",
    "4.  Remove Rows with all Null Values<br>\n",
    "5.  Remove Columns with all Null Values<br>\n",
    "6.  Apply Business Logic to Create Simple Data Sets<br>\n",
    "7.  Rename Coded Values to be Meaningful and easily Grasped<br>\n",
    "8.  Round Meaningless Decimals<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>well_id</th>\n",
       "      <th>GR</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>99.005617</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>105.563944</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>105.921065</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>113.513354</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>101.523783</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id  well_id          GR  label\n",
       "0       0        0   99.005617      0\n",
       "1       1        0  105.563944      0\n",
       "2       2        0  105.921065      0\n",
       "3       3        0  113.513354      0\n",
       "4       4        0  101.523783      0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, view your data\n",
    "trainDF.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>well_id</th>\n",
       "      <th>GR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5000</td>\n",
       "      <td>134.943504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "      <td>127.004675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5000</td>\n",
       "      <td>133.159255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5000</td>\n",
       "      <td>134.411762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5000</td>\n",
       "      <td>135.748644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id  well_id          GR\n",
       "0       0     5000  134.943504\n",
       "1       1     5000  127.004675\n",
       "2       2     5000  133.159255\n",
       "3       3     5000  134.411762\n",
       "4       4     5000  135.748644"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDF.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row_id       int64\n",
      "well_id      int64\n",
      "GR         float64\n",
      "label        int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Check the Data Types of your Data and make sure they match what you expect\n",
    "print(trainDF.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row_id       int64\n",
      "well_id      int64\n",
      "GR         float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(testDF.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Select columns that you intend to use in your model and order them correctly\n",
    "testDFColumns = testDF[['row_id','well_id','GR']]\n",
    "trainDFColumns = trainDF[['row_id','well_id','GR','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.  Rename your columns according to a standard.  I prefer camel caps.\n",
    "testDFColumns.columns = ['RowID','WellID','GR']\n",
    "trainDFColumns.columns =  ['RowID','WellID','GR','Label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign Correct Data Types\n",
    "\n",
    "It is important to assign correct data types to your data and to remove values which do not fit, filling them with nulls. <br>\n",
    "Do not remove rows with a few missing values at this stage, as this information may be valuable to data scientists during the data transformation stage.\n",
    "\n",
    "Click this link for a detailed blog post on assigning data types in Python: https://pbpython.com/pandas_dtypes.html\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Assign correct data types to each column\n",
    "# 3. Convert incorrect data to nulls (Numeric)\n",
    "\n",
    "# Test Data\n",
    "testDFColumns['RowID'] = pd.to_numeric(testDFColumns['RowID'], errors='coerce')\n",
    "testDFColumns['WellID'] = pd.to_numeric(testDFColumns['WellID'], errors='coerce')\n",
    "testDFColumns['GR'] = pd.to_numeric(testDFColumns['GR'], errors='coerce')\n",
    "# Train Data\n",
    "trainDFColumns['RowID'] = pd.to_numeric(trainDFColumns['RowID'], errors='coerce')\n",
    "trainDFColumns['WellID'] = pd.to_numeric(trainDFColumns['WellID'], errors='coerce')\n",
    "trainDFColumns['GR'] = pd.to_numeric(trainDFColumns['GR'], errors='coerce')\n",
    "trainDFColumns['Label'] = trainDFColumns['Label'].astype('category')\n",
    "\n",
    "# Other Useful Python Functions\n",
    "# pd.to_datetime(df[['<MyDateColumn>']], errors='coerce') for DateTime columns\n",
    "# df[\"TrueFalseColumn\"] = np.where(df[\"TrueFalseColumn\"] == \"Y\", True, False) for Boolean (Yes / No, T / F) Columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RowID     0\n",
      "WellID    0\n",
      "GR        0\n",
      "dtype: int64\n",
      "RowID     0\n",
      "WellID    0\n",
      "GR        0\n",
      "Label     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 3. Convert incorrect data to nulls (Categorical and DateTime)\n",
    "\n",
    "# Make a function(s) that converts unacceptable values to Nulls\n",
    "\n",
    "# In this case, the only acceptable values are 0 and 1\n",
    "def MyLabelColumnCategoryCheck(x):\n",
    "    if x == 0:\n",
    "        return 0\n",
    "    elif x == 1:\n",
    "        return 1\n",
    "    elif x == 2:\n",
    "        return 2\n",
    "    elif x == 3:\n",
    "        return 3\n",
    "    elif x == 4:\n",
    "        return 4\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    \n",
    "# Train Data    \n",
    "trainDFColumns['Label'] = trainDFColumns['Label'].apply(MyLabelColumnCategoryCheck)\n",
    "\n",
    "# Check Nulls\n",
    "print(testDFColumns.isna().sum())\n",
    "print(trainDFColumns.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Null Rows and Columns\n",
    "\n",
    "Rows that are completely Null and Columns that are completely Null hold no value and should be removed.\n",
    "\n",
    "Click this link for a detailed blog post on Null removal in Python: https://www.journaldev.com/33492/pandas-dropna-drop-null-na-values-from-dataframe\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Remove Rows with all Null Values\n",
    "\n",
    "# Test Data\n",
    "testDFNullRemoved = testDFColumns.dropna(how='all')\n",
    "# Train Data\n",
    "trainDFNullRemoved = trainDFColumns.dropna(how='all')\n",
    "\n",
    "# 5. Remove Columns with all Null Values\n",
    "\n",
    "# Test Data\n",
    "testDFNulLRemoved = testDFNullRemoved.dropna(how='all', axis = 1)\n",
    "# Train Data\n",
    "trainDFNullRemoved = trainDFNullRemoved.dropna(how='all', axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Business Logic to Make Simplified Data Sets\n",
    "\n",
    "Each data set is going to have its own business logic that can get very confusing.  It is important that new employees can grasp the meaning of your Data.<br>  In order to do so, follow the advice below: <br>\n",
    "<br>\n",
    "1.  If Business Months, Weeks and Years do not correspond to their calendar dates, create new columns to capture that information.<br>\n",
    "2.  If a single column contains multiple measurements, such as centimeters and inches, create a column to capture that information.<br>\n",
    "3.  If a table contains multiple grains of data, such as rows containing aggregates for both day and month, split the table by grain into new tables.<br>\n",
    "4.  If the same value in the same column has multiple meanings based on a second column, change the original column to have different values. <br>\n",
    "5.  Rearrange the data so that most questions can be answered with simple queries.<br>For example, you should always be able to get an accurate sales number by summing the sales column without involving a second column.<br>\n",
    "6.  There are many other similar transformations you should do, always with the goal of simplicity.<br>\n",
    "7.  Test your data by showing it to someone who knows SQL but not your data and ask them to solve simple business questions.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.  Apply Business Logic \n",
    "\n",
    "# Apply your own logic here\n",
    "\n",
    "# Test Data\n",
    "testDFLogicApplied = testDFNullRemoved\n",
    "# Train Data\n",
    "trainDFLogicApplied = trainDFNullRemoved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Rename coded values to be meaningful and easily grasped\n",
    "\n",
    "# Make a function that converts coded values to readable values\n",
    "\n",
    "# In this case, we assign names to the label which correspond to sediment layer types\n",
    "def MyLabelColumnCodeReader(x):\n",
    "    if x == 0:\n",
    "        return 'Default Layer'\n",
    "    elif x == 1:\n",
    "        return 'Layer 1'\n",
    "    elif x == 2:\n",
    "        return 'Layer 2'\n",
    "    elif x == 3:\n",
    "        return 'Layer 3'\n",
    "    elif x == 4:\n",
    "        return 'Layer 4'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    \n",
    "# Train Data\n",
    "trainDFLogicApplied['Label']  = trainDFLogicApplied['Label'].apply(MyLabelColumnCodeReader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Round Meaningless Decimals.\n",
    "# Meaningless Decimals cost companies a lot of money.  Round them to save cash and compute time.\n",
    "\n",
    "# Round each of your numerical columns.  In this case, we will round to 1 decimal.\n",
    "\n",
    "# Test Data\n",
    "testRefinedDF = testDFLogicApplied.round({'GR': 1})\n",
    "# Train Data\n",
    "trainRefinedDF = trainDFLogicApplied.round({'GR': 1})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write your refined data back to your Data Lake\n",
    "After refining your data, save it to a place on a data lake where it is accessible to all of the data workers who require access.<br>\n",
    "Make sure that you indicate in the folder path and file name that it is refined, curated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Workspace Name</th>\n",
       "      <td>ancient-rivers-ml-workspace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Datastore Name</th>\n",
       "      <td>ancientrivers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Container Name</th>\n",
       "      <td>dms-data-lake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Resource Group</th>\n",
       "      <td>xeek-ancient-rivers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Storage Account</th>\n",
       "      <td>dmsdatalake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            \n",
       "Workspace Name   ancient-rivers-ml-workspace\n",
       "Datastore Name   ancientrivers              \n",
       "Container Name   dms-data-lake              \n",
       "Resource Group   xeek-ancient-rivers        \n",
       "Storage Account  dmsdatalake                "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output a nice table with all of your essential datastore information so you have it on hand\n",
    "dsoutput = {}\n",
    "dsoutput['Workspace Name'] = ws.name\n",
    "dsoutput['Datastore Name'] = datastore.name\n",
    "dsoutput['Container Name'] = datastore.container_name\n",
    "dsoutput['Resource Group'] = ws.resource_group\n",
    "dsoutput['Storage Account'] = datastore.account_name\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "dsoutputDf = pd.DataFrame(data = dsoutput, index = [''])\n",
    "dsoutputDf.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<azure.storage.blob.models.ResourceProperties at 0x7f4580da0470>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Data\n",
    "\n",
    "# Create the BlockBlockService that the system will use to write data.  \n",
    "# Specify your datastore storage account and account key.\n",
    "block_blob_service = BlockBlobService(\n",
    "    account_name='dmsdatalake', account_key='pu0OWdEM2rqecJBhIZoLqPMo/DLl3JQQ3eryF1HJVbejjLyUFqoOlVIQ4rv24v+1xlJZjRv9x1Ix4ucTqZqWKw==')\n",
    "\n",
    "# Specify your container in your storage account where you wish to save Refined Data.\n",
    "# It can be different from the container used in your primary datastore, in which case register it as another datastore\n",
    "container_name = 'dms-data-lake'\n",
    "\n",
    "# Change your Pandas Dataframe to CSV\n",
    "testRefinedCSV = testRefinedDF.to_csv(index=False)\n",
    "\n",
    "# Assign a path and filename inside your container in your storage account\n",
    "data_lake_file_path =  \"dev/refined/test/ancient-rivers-test\"\n",
    "\n",
    "# Upload the CSV into your storage account\n",
    "block_blob_service.create_blob_from_text(\n",
    "    container_name, data_lake_file_path,testRefinedCSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<azure.storage.blob.models.ResourceProperties at 0x7f4580d93c18>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train Data\n",
    "\n",
    "# Create the BlockBlockService that the system will use to write data.  \n",
    "# Specify your datastore storage account and account key.\n",
    "block_blob_service = BlockBlobService(\n",
    "    account_name='dmsdatalake', account_key='pu0OWdEM2rqecJBhIZoLqPMo/DLl3JQQ3eryF1HJVbejjLyUFqoOlVIQ4rv24v+1xlJZjRv9x1Ix4ucTqZqWKw==')\n",
    "\n",
    "# Specify your container in your storage account where you wish to save Refined Data.\n",
    "# It can be different from the container used in your primary datastore, in which case register it as another datastore\n",
    "container_name = 'dms-data-lake'\n",
    "\n",
    "# Change your Pandas Dataframe to CSV\n",
    "trainRefinedCSV = trainRefinedDF.to_csv(index=False)\n",
    "\n",
    "# Assign a path and filename inside your container in your storage account\n",
    "data_lake_file_path =  \"dev/refined/train/ancient-rivers-train\"\n",
    "\n",
    "# Upload the CSV into your storage account\n",
    "block_blob_service.create_blob_from_text(\n",
    "    container_name, data_lake_file_path,trainRefinedCSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register your Refined Datasets\n",
    "This allows you to share your Curated Datasets with others in your workspace, to version and keep track of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you stored your Refined Datasets in a separate container, first create another Datastore for that container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the files and/or directories in your datastore.  You can pull in multiple or single files.\n",
    "# Here, we pull in testing data and training data separately\n",
    "\n",
    "datastore_path_refined_test = [\n",
    "                  (datastore,  \"dev/refined/test/ancient-rivers-test\")\n",
    "                 ]\n",
    "\n",
    "datastore_path_refined_train = [\n",
    "                  (datastore,  \"dev/refined/train/ancient-rivers-train\")\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tabular Data Sets\n",
    "test_data_refined = Dataset.Tabular.from_delimited_files(path=datastore_path_refined_test)\n",
    "train_data_refined = Dataset.Tabular.from_delimited_files(path=datastore_path_refined_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"source\": [\n",
       "    \"('ancientrivers', 'dev/refined/test/ancient-rivers-test')\"\n",
       "  ],\n",
       "  \"definition\": [\n",
       "    \"GetDatastoreFiles\",\n",
       "    \"ParseDelimited\",\n",
       "    \"DropColumns\",\n",
       "    \"SetColumnTypes\"\n",
       "  ],\n",
       "  \"registration\": {\n",
       "    \"id\": \"d922ed97-3d6e-45b7-b804-fc7c3b8594b7\",\n",
       "    \"name\": \"ancient-rivers-test-refined\",\n",
       "    \"version\": 1,\n",
       "    \"description\": \"Curated Ancient Rivers Testing Data.  No Label Included\",\n",
       "    \"tags\": {\n",
       "      \"Type\": \"Classification\",\n",
       "      \"Project\": \"Ancient Rivers\"\n",
       "    },\n",
       "    \"workspace\": \"Workspace.create(name='ancient-rivers-ml-workspace', subscription_id='47a7ec0c-37ad-428b-9114-b87ea1057632', resource_group='xeek-ancient-rivers')\"\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Register and tag your data sets\n",
    "test_data_refined.register(workspace=ws,\n",
    "                        name=\"ancient-rivers-test-refined\",\n",
    "                        description=\"Curated Ancient Rivers Testing Data.  No Label Included\",\n",
    "                        tags = {\"Type\": \"Classification\", \"Project\": \"Ancient Rivers\"},\n",
    "                        create_new_version=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"source\": [\n",
       "    \"('ancientrivers', 'dev/refined/train/ancient-rivers-train')\"\n",
       "  ],\n",
       "  \"definition\": [\n",
       "    \"GetDatastoreFiles\",\n",
       "    \"ParseDelimited\",\n",
       "    \"DropColumns\",\n",
       "    \"SetColumnTypes\"\n",
       "  ],\n",
       "  \"registration\": {\n",
       "    \"id\": \"8f424c06-72ca-47d8-9c8d-2dffd629da73\",\n",
       "    \"name\": \"ancient-rivers-train-refined\",\n",
       "    \"version\": 1,\n",
       "    \"description\": \"Curated Ancient Rivers Training Data.  Label Included\",\n",
       "    \"tags\": {\n",
       "      \"Type\": \"Classification\",\n",
       "      \"Project\": \"Ancient Rivers\"\n",
       "    },\n",
       "    \"workspace\": \"Workspace.create(name='ancient-rivers-ml-workspace', subscription_id='47a7ec0c-37ad-428b-9114-b87ea1057632', resource_group='xeek-ancient-rivers')\"\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_refined.register(workspace=ws,\n",
    "                        name=\"ancient-rivers-train-refined\",\n",
    "                        description=\"Curated Ancient Rivers Training Data.  Label Included\",\n",
    "                        tags = {\"Type\": \"Classification\", \"Project\": \"Ancient Rivers\"},\n",
    "                        create_new_version=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
