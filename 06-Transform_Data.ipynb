{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform your Data\n",
    "The point of transforming your data is to make it as easy as possible for your algorithms to distinguish signal from noise.  Each data science project will require different transformations and creative transformations are often the key to a successful AI project.  First, load in your Workspace, Datastore and Datasets.<br><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Azure Libaries\n",
    "from azureml.core import Datastore\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "from azure.storage.blob import BlockBlobService\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve your workspace by name by filling in the lower case values between double quotes\n",
    "ws = Workspace.get(name=\"<my-workspace>\",\n",
    "        subscription_id=\"<my-azure-subscription-id>\",\n",
    "        resource_group=\"<my-resource-group>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve your Datastore by name by filling in the lower case values between double quotes\n",
    "datastore_name = \"<my-datastore-name>\"\n",
    "datastore = Datastore.get(ws, datastore_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve your Refined Datasets by name by filling in the lower case values between double quotes\n",
    "dataset_name_test = \"<my-refined-dataset-name>\"\n",
    "dataset_name_train  = \"<my-refined-dataset-name>\"\n",
    "\n",
    "# Load Data in as Tabular Datasets\n",
    "testing_data  = Dataset.get_by_name(ws, dataset_name_test, version='latest')\n",
    "training_data = Dataset.get_by_name(ws, dataset_name_train, version='latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert your tabular dataset to pandas data frames\n",
    "testDF = testing_data.to_pandas_dataframe()\n",
    "trainDF = training_data.to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# It's time to start working with your data!\n",
    "This next step should be customized to fit each project based on your understanding of the problem and the data.<br>\n",
    "You should understand your data, transform your data, and save it as a Dataset so everyone on your workspace can access it.<br>\n",
    "\n",
    "Common transformations include, but are not limited to:<br>\n",
    "- Rebalancing your Dependent Variable for Classification Problems (The column you are trying to predict)\n",
    "- Transforming your Dependent Variable to follow a Normal Distribution for Regression Problems\n",
    "- Removing outliers\n",
    "- Smoothing Noisy Numeric Indepedent Variables (IVs)\n",
    "- Binning Numeric IVs\n",
    "- Binning Categorical IVs\n",
    "- Create New Columns based on your refined data\n",
    "- Impute Null Values across the Dataset\n",
    "- Create New Columns to indicate where Null Values existed before imputation for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since each transformation is problem specific, you will have to research your data to determine what is appropriate.\n",
    "# For an example, looked at the filled version of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your own transformation code here. The file lines should be\n",
    "testTransformedDF = \n",
    "trainTransformedDF = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write your Transformed Data back to your Data Lake\n",
    "After transforming your data, save it to a place on a data lake where it is accessible to all of the data workers who require access.<br>\n",
    "Make sure that you indicate in the folder path and file name that it is transformed data for a specific data science project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output a nice table with all of your essential datastore information so you have it on hand\n",
    "dsoutput = {}\n",
    "dsoutput['Workspace Name'] = ws.name\n",
    "dsoutput['Datastore Name'] = datastore.name\n",
    "dsoutput['Container Name'] = datastore.container_name\n",
    "dsoutput['Resource Group'] = ws.resource_group\n",
    "dsoutput['Storage Account'] = datastore.account_name\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "dsoutputDf = pd.DataFrame(data = dsoutput, index = [''])\n",
    "dsoutputDf.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data\n",
    "\n",
    "# Create the BlockBlockService that the system will use to write data.  \n",
    "# Specify your datastore storage account and account key.\n",
    "block_blob_service = BlockBlobService(\n",
    "    account_name='<my-storage-account>', account_key='<my-storage-account-key>')\n",
    "\n",
    "# Specify your container in your storage account where you wish to save Transformed Data.\n",
    "# It can be different from the container used in your primary datastore, in which case register it as another datastore\n",
    "container_name = '<my-container>'\n",
    "\n",
    "# Change your Pandas Dataframe to CSV\n",
    "testTransformedCSV = testTransformedDF.to_csv(index=False)\n",
    "\n",
    "# Assign a path and filename inside your container in your storage account\n",
    "data_lake_file_path = \"<my-path-on-datastore-to-transformed-test-data>\"\n",
    "\n",
    "# Upload the CSV into your storage account\n",
    "block_blob_service.create_blob_from_text(\n",
    "    container_name, data_lake_file_path,testTransformedCSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Data\n",
    "\n",
    "# Create the BlockBlockService that the system will use to write data.  \n",
    "# Specify your datastore storage account and account key.\n",
    "block_blob_service = BlockBlobService(\n",
    "    account_name='<my-storage-account>', account_key='<my-storage-account-key>')\n",
    "\n",
    "# Specify your container in your storage account where you wish to save Transformed Data.\n",
    "# It can be different from the container used in your primary datastore, in which case register it as another datastore\n",
    "container_name = '<my-container>'\n",
    "\n",
    "# Change your Pandas Dataframe to CSV\n",
    "trainTransformedCSV = trainTransformedDF.to_csv(index=False)\n",
    "\n",
    "# Assign a path and filename inside your container in your storage account\n",
    "data_lake_file_path = \"<my-path-on-datastore-to-transformed-train-data>\"\n",
    "\n",
    "# Upload the CSV into your storage account\n",
    "block_blob_service.create_blob_from_text(\n",
    "    container_name, data_lake_file_path, trainTransformedCSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register your Transformed Datasets\n",
    "This allows you to share your Transformed Datasets with others in your workspace, to version and keep track of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you stored your Transformed Datasets in a separate container, first create another Datastore for that container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datastore' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7acee4f379c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m datastore_path_transformed_test = [\n\u001b[0;32m----> 5\u001b[0;31m                   \u001b[0;34m(\u001b[0m\u001b[0mdatastore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"<my-path-on-datastore-to-transformed-test-data>\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m                  ]\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datastore' is not defined"
     ]
    }
   ],
   "source": [
    "# Specify the files and/or directories in your datastore.  You can pull in multiple or single files.\n",
    "# Here, we pull in testing data and training data separately\n",
    "\n",
    "datastore_path_transformed_test = [\n",
    "                  (datastore, \"<my-path-on-datastore-to-transformed-test-data>\")\n",
    "                 ]\n",
    "\n",
    "datastore_path_transformed_train = [\n",
    "                  (datastore, \"<my-path-on-datastore-to-transformed-train-data>\")\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tabular Data Sets\n",
    "test_data_transformed = Dataset.Tabular.from_delimited_files(path=datastore_path_transformed_test)\n",
    "train_data_transformed = Dataset.Tabular.from_delimited_files(path=datastore_path_transformed_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register and tag your data sets\n",
    "test_data_transformed.register(workspace=ws,\n",
    "                        name=\"<my-dataset-name>\",\n",
    "                        description=\"<my-dataset-description>\",\n",
    "                        tags = {\"<my-tag-name>\": \"<my-tag-value>\", \"<my-tag-name2>\": \"<my-tag-value2>\"},\n",
    "                        create_new_version=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register and tag your data sets\n",
    "train_data_transformed.register(workspace=ws,\n",
    "                        name=\"<my-dataset-name>\",\n",
    "                        description=\"<my-dataset-description>\",\n",
    "                        tags = {\"<my-tag-name>\": \"<my-tag-value>\", \"<my-tag-name2>\": \"<my-tag-value2>\"},\n",
    "                        create_new_version=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>\n",
    "\n",
    "\n",
    "\n",
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
